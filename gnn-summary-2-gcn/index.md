# 图神经网络总结(二) —— 图卷积网络


卷积在深度学习中的应用很广泛，最开始接触深度学习的时候，一直认为卷积神经网络是专门用于图像特征提取的，实际上卷积神经网络最主要的优点在于其局部连接的特性，能有效减少网络参数，如果只关注图像的一个局部，对其进行卷积实际上就相当于通过了一个全连接层，而图像的每一个局部都共享了这个全连接；另一方面，对于图像、音频这种更加关注局部特征的数据，卷积神经网络这种局部的特性恰好与之对应，因此能够提升网络的表现；此外，卷积神经网络还具有平移不变性（这种平移不变性似乎不是在所有情况下都成立？），同样有助于特征提取。

在欧几里得数据上进行卷积通常是有意义的，例如对图像上局部的特征、句子中词与词之间的关系进行抽取。由于数据是在欧几里得平面整齐排列的，我们只需要用固定大小的卷积核进行卷积，但对于非欧数据，卷积的操作应该如何定义呢？

## 2 谱域卷积

谱域卷积顾名思义是在图(graph)的谱域进行卷积，为此我们需要一个将图从顶点域(Vertex Domain)变换到谱域(Spectral Domain)的方法，在谱域进行卷积操作后再变换到顶点域。

### 2.1 图的傅里叶变换

在数字信号处理领域，傅里叶变换被用来将信号从时域变换到频域分析，这里我们定义一个函数 $ f: V \longrightarrow R $ (其中 $V$ 表示图上顶点的集合)来表示图信号 ，$f(i)$ 表示第 $i$ 个顶点的信号，同样的，我们也可以定义图上的傅里叶变换，将图信号从顶点域变换到谱域。

这里首先简单回顾一下传统的傅里叶变换：

$$ \mathscr{F}(\omega)=\int_{-\infty}^\infty f(t) e^{-i \omega t}dt \tag 1$$ 

傅里叶变换的思想是将函数用一系列基( $ e^{-i \omega t} $ )表示。

对于图信号，我们可以将其看成一个维度为 $N$ 的向量，即 $ f \in R^N $ ，其中 $N$ 表示顶点集的大小，这里我们使用拉普拉斯矩阵的特征向量作为基( $N$ 个相互正交的 $N$ 维向量可以表示 $N$ 维空间中的任意向量)。

用 $\lambda_l$ 和 $u_l$ 分别表示第 $l$ 个特征值和对应的特征向量， $\hat{f}({\lambda_l})$ 表示第 $l$ 个特征值对应的特征向量的系数，则有：

$$ f(i) = \sum^{N-1}_{l=0} \hat{f}({\lambda_l})u_l \tag 2$$

即：

$$ f = U\hat{f} \tag 3$$

因此我们可以得到图上的傅里叶变换：

$$ \hat{f} = U^Tf \tag 4 $$

这里先提出几点疑问放在后面讨论：

- 为什么要使用拉普拉斯矩阵的特征值作为基底（拉普拉斯算子之于傅里叶变换，图的结构）
- 

### 2.2 谱域卷积



## 参考

[[1]. The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains](https://arxiv.org/abs/1211.0053)

[[2]. Graph Convolutions and Machine Learning](https://dash.harvard.edu/handle/1/38811540)

[[3]. Wiki: Laplacian Matrix](https://en.wikipedia.org/wiki/Laplacian_matrix)


